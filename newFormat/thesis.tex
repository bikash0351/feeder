\documentclass[oneside, 12pt]{book}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Packages %%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{array}
\usepackage{tocloft}
\usepackage{hyperref}
\urlstyle{same}
\usepackage{titlesec}
\usepackage{mathtools}
\usepackage{twoopt}
\usepackage{caption}
\usepackage{float}
\usepackage{gensymb}
\usepackage[style=numeric-comp, backend=biber, sorting=none]{biblatex}
\addbibresource{references.bib}
\renewcommand{\cftdotsep}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Document Dimentions Size %%%%%%%%%%%%%%%%%
\geometry{a4paper, total={210mm, 297mm}, left=1.5in, top=1in, right = 1in, bottom = 1.59in, footskip = 0.59in}
\captionsetup{compatibility=false}
\def\UrlBreaks{\do\/\do-}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%% Figure referencing with "Figure" hyperref %%%%%%
\newcommandtwoopt*{\myref}[3][][]{%
  \hyperref[{#3}]{%
    \ifx\\#1\\%
    \else
      #1~%
    \fi
    \ref*{#3}%
    \ifx\\#2\\%
    \else
      \,#2%
    \fi
  }%
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%% Figure 2.1 vs Figure 2.1: %%%%%%%%%%%%%%%
\captionsetup[figure]{labelsep=space}
\captionsetup[table]{labelsep=space}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%% REMOVE GAP AFTER TOC, LOF, LOT %%%%%%%
\setlength\cftafterloftitleskip{24pt}
\setlength\cftafterlottitleskip{24pt}
\setlength\cftaftertoctitleskip{24pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Equation numbering style %%%%%%%%%%%%%
\numberwithin{equation}{chapter}
\counterwithin{equation}{chapter}	% Chapter wise
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%% Figure and Table numbering style %%%%%%%
%\counterwithout{figure}{chapter}	% Continuous
%\counterwithout{table}{chapter}	% Continuous
\counterwithin{figure}{chapter}		% Chapter wise
\counterwithin{table}{chapter}		% Chapter wise
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\sizexii}{\fontsize{12pt}{6pt}\selectfont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titleformat{\section}{\sizexii\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\sizexii\bfseries}{\thesubsection}{1em}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Colors of text %%%%%%%%%%%%%%%%%%%%
%\usepackage{hyperref}
%\hypersetup{
%    colorlinks=true,
%    citecolor=black,
%    filecolor=black,
%    linkcolor=black,
%    urlcolor=black,
%    anchorcolor=black
%}
\hypersetup{
  colorlinks = true,
  linkcolor = black,
  anchorcolor = [rgb]{0.05098,0.50588,0.67451},
  citecolor = [rgb]{0.05098,0.50588,0.67451},
  urlcolor = [rgb]{0.05098,0.50588,0.67451}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\let\cleardoublepage\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%% Section wise indetnt off (a) %%%%%%%%%%%%
\setlength{\cftsecindent}{0pt}% Remove indent for \section
\setlength{\cftsubsecindent}{0pt}% Remove indent for \subsection
\cftsetindents{section}{1em}{3em}
\cftsetindents{subsection}{1em}{3em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% For dots at chapter heading in TOC %%%%%%%%

\makeatletter
\renewcommand{\@dotsep}{2}
\renewcommand*\l@chapter[2]{%
  \ifnum \c@tocdepth >\m@ne
    \addpenalty{-\@highpenalty}%
    \vskip 1.0em \@plus\p@
    \setlength\@tempdima{1.5em}%
    \begingroup
      \parindent \z@ \rightskip \@pnumwidth
      \parfillskip -\@pnumwidth
      \leavevmode \bfseries
      \advance\leftskip\@tempdima
      \hskip -\leftskip
      #1\nobreak\normalfont\leaders\hbox{$\m@th
        \mkern \@dotsep mu\hbox{.}\mkern \@dotsep
        mu$}\hfill\nobreak\hb@xt@\@pnumwidth{\hss #2}\par
      \penalty\@highpenalty
    \endgroup
  \fi}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%% Remove chapter indent in TOC %%%%%%%%%%
\newcounter{mysection}
\renewcommand\themysection{\Alph{mysection}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\ToToC[1]{\addcontentsline{toc}{section}   {\hspace*{4em}\appendixname~\themysection\hspace*{1em}#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%% Section wise indetnt off (b) %%%%%%%%%%%%
\makeatletter
\newcommand\mysection{\refstepcounter{mysection}%
  \@startsection{paragraph}{4}{\z@}%
  {-3.5ex \@plus -1ex \@minus -.2ex}%
  {2.3ex \@plus.2ex}%
  {\normalfont\Large\bfseries}}
\renewcommand*\l@section{\@dottedtocline{1}{0em}{4em}}%
\renewcommand*\l@subsection{\@dottedtocline{2}{1em}{4em}}%
\renewcommand*\l@subsubsection{\@dottedtocline{3}{0em}{4em}}%
\newcommand*\l@mysection{\@dottedtocline{4}{1em}{1em}}%
\renewcommand\appendix{\par
  \setcounter{section}{0}%
  \setcounter{subsection}{0}%
  \renewcommand\theparagraph{\Alph{mysection}}
  \renewcommand\themysection{\@Alph\c@paragraph}}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%% Dot after section 1.1., 1.2.3. %%%%%%%%
\renewcommand{\thesection}{\arabic{chapter}.\arabic{section}.{}}
\renewcommand{\thesubsection}{\arabic{chapter}.\arabic{section}.\arabic{subsection}.{}}
\renewcommand{\thesubsubsection}{\arabic{chapter}.\arabic{section}.\arabic{subsection}.\arabic{subsubsection}.{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\parindent}{0pt}
\pagestyle{plain}
\linespread{1.5}
\setlength{\parskip}{6pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%
%%%%%%%%%%%%%%%%%%%%%% DOCUMENTATION FOR SPINE %%%%%%%%%%%%%%%%%%%
%\usepackage{everypage}
%
%\def\PageTopMargin{1in}
%\def\PageLeftMargin{1.5in}
%\newcommand\atxy[3]{%
% \AddThispageHook{\smash{\hspace*{\dimexpr-\PageLeftMargin-\hoffset+#1\relax}%
%  \raisebox{\dimexpr\PageTopMargin+\voffset-#2\relax}{#3}}}}
%
%\setlength{\fboxrule}{0pt} % No boarder in Fbox inserted
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% STARTING OF THE DOCUMENT %%%%%%%%%%%%%

\begin{document}

\titlespacing*{\section}{0pt}{18pt}{6pt}
\titlespacing*{\subsection}{0pt}{18pt}{6pt}
\titlespacing*{\subsubsection}{0pt}{18pt}{6pt}
\titlespacing*{\subsubsubsection}{0pt}{6pt}{6pt}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% TITLE PAGE WITH SPINE %%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT in SPINE %%%%%%%%%%%%%%%%%%%%%%%
%\atxy{0.65in}{0.25in}{\rotatebox[origin=l]{-90}{\fbox{\makebox[11in]{\normalfont
%\textbf{EDM00/075} 
%\hspace{0.75cm} 
%\textbf{Title of your thesis work} 
%\hspace{0.75cm} 
%\textbf{Author's Name} 
%\hspace{0.75cm} 
%\textbf{2021} 
%\rule[-12pt]{0pt}{25pt}}}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{titlepage}
%\centering
%
%\includegraphics[width = 3.5cm, height = 3.5cm]{figures/logos/tu_logo.png}
%
%\begin{doublespacing}
%
%\bigskip
%
%TRIBHUVAN UNIVERSITY\\
%INSTITUTE OF ENGINEERING\\
%THAPATHALI CAMPUS
%
%\bigskip
%\bigskip
%%\bigskip
%
%\begin{flushleft} EDM 00/075 \end{flushleft}
%
%\textbf{Title of your thesis work}
%
%\bigskip
%\bigskip
%%\bigskip
%
%by
%
%%\bigskip
%\bigskip
%\bigskip
%
%\textbf{Author's Name}
%
%\bigskip
%\bigskip
%%\bigskip
%
%A THESIS\\
%SUBMITTED TO T
%
%\bigskip
%\bigskip
%%\bigskip
%
%DEPARTMENT OF AUTOMOBILE AND MECHANICAL ENGINEERING\\
%KATHMANDU, NEPAL\\
%
%\bigskip
%\bigskip
%
%SEPTEMBER, 2021 \\
%
%\end{doublespacing}
%\end{titlepage}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% END OF TITLE PAGE WITH SPINE %%%%%%%%%%%%%%%%%%%%
%
%\clearpage
%\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
\centering

\includegraphics[width = 3.5cm, height = 3.8cm]{tu_logo.png}

\begin{doublespacing}

\bigskip

TRIBHUVAN UNIVERSITY\\
INSTITUTE OF ENGINEERING\\
PULCHOWK CAMPUS\\

\bigskip
\bigskip

\textbf{SHORT-TERM ELECTRICAL LOAD FORECASTING FOR
	BANESHWOR FEEDER USING MACHINE AND DEEP LEARNING MODELS}

\bigskip
\bigskip

Submitted by

\bigskip

%			..... \\
%			.....
SUJIT KOIRALA \\
(PUL075MSPSE016)

\bigskip
\bigskip

A THESIS REPORT\\
SUBMITTED TO THE DEPARTMENT OF ELECTRICAL ENGINEERING IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE
DEGREE OF MASTER OF SCIENCE IN
POWER SYSTEM ENGINEERING

\bigskip
\bigskip

DEPARTMENT OF ELECTRICAL ENGINEERING \\ PULCHOWK CAMPUS, LALITPUR, NEPAL\\

\bigskip
\bigskip

JANUARY, 2026 \\

\end{doublespacing}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% END OF TITLE PAGE %%%%%%%%%%%%%%%%%%%%

\clearpage
\newpage

\pagenumbering{roman}

%%% Add here the file from misc page file
\section*{\begin{center}  {\sizexii COPYRIGHT} \end{center}}
\phantomsection \addcontentsline{toc}{section}{Copyright}\vspace{-16pt}
This thesis may be accessed for academic and research purposes at the Library, Department of Electrical Engineering, Pulchowk Campus, and Institute of Engineering. Permission for substantial reproduction of this report for scholarly use may be granted by the supervisors or, if unavailable, by the Head of Department. Proper acknowledgment must be given to the author and the Department of Electrical Engineering, Pulchowk Campus, Institute of Engineering, for any use of the material herein. Any reproduction or use of this report for commercial purposes without written consent from both the Department and the author is strictly prohibited. Requests for such permissions should be directed to:

\bigskip
\bigskip
Head \\
Department of Electrical Engineering \\
Pulchowk Campus, Institute of Engineering \\
Pulchowk, Lalitpur \\
Nepal
\newpage
\newgeometry{a4paper, total={210mm, 297mm}, left=1.5in, top=1in, right = 1in, bottom = 1in, footskip = 0in}

%%% Approval page
\phantomsection \addcontentsline{toc}{section}{Approval page}
\vspace*{2.5cm}% Space for college letterhead
{\centering
    \textbf{Certificate of Approval}\\}
\noindent The undersigned certify that they have read and recommended to the Institute of Engineering for acceptance, a THESIS entitled \textbf{Short-Term Electrical Load Fore
	casting for Baneshwor Feeder Using Machine and Deep Learning Models} submitted by Sujit Koirala (PUL075MSPSE016) as a partial requirement for the Master of Science in Power System Engineering. After review, we recommend its acceptance by the Institute of Engineering.

\bigskip

\begin{table}[h]
    \centering
    \begin{tabular}{p{7cm} p{7cm}}
        
        % --- ROW 1 ---
        % 1. Create space for the signature (ink)
        & \\[0.8cm] 
        % 2. The Line (reduced space after to keep name close)
        \rule{7cm}{0.5pt} & \rule{7cm}{0.5pt} \\[5pt] 
        % 3. The Details
        Amrit Dhakal & Deependra Neupane \\
        Supervisor & Supervisor \\
        Assistant Professor, Department of Electrical Engineering & Assistant Professor, Department of Electrical Engineering \\
        Pulchowk Campus, IOE, Tribhuvan University & Pulchowk Campus, IOE, Tribhuvan University \\

        % --- ROW 2 ---
        % 1. Create space for the signature (ink)
        & \\[0.8cm]
        % 2. The Line
        \rule{7cm}{0.5pt} & \rule{7cm}{0.5pt} \\[5pt]
        % 3. The Details
        Dr. Kamal Chapagain & Dr. Bishal Silwal \\
        External Examineer, Department of Electrical and Electronics Engineering & Program Coordinator, Msc. in Power System Engineering \\
        Kathmandu University & Pulchowk Campus, IOE, Tribhuvan University \\

        % --- ROW 3 ---
        % 1. Create space for the signature (ink)
        & \\[0.8cm]
        % 2. The Line
        \rule{7cm}{0.5pt} &  \\[5pt]
        % 3. The Details
        Assoc. Prof. Jeetendra Chaudhary &  \\
        HoD, Department of Electrical Engineering &  \\
        Pulchowk Campus, IOE, Tribhuvan University & \\
    \end{tabular}
\end{table}

Date: January, 2026

\restoregeometry
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% ABSTRACT PAGE %%%%%%%%%%%%%%%%%%

\section*{\begin{center} {ABSTRACT} \end{center}}
\phantomsection
\addcontentsline{toc}{section}{Abstract} \vspace{-16pt}

Predicting electricity demand a few hours ahead matters a lot for running power grids smoothly and for trading energy in markets. Because load patterns shift with weather, daily habits, and seasons, older statistical tools often miss the complex, nonlinear behavior in the data. This thesis tackles hour-ahead load prediction for the Baneshwor Feeder by testing several machine learning and deep learning methods side by side. The raw data---hourly megawatt readings together with temperature, solar radiation, and humidity---went through careful cleaning: gaps were filled, outliers removed, time-based features extracted, and cyclical hour/month values encoded as sine-cosine pairs. Six ML models (Linear Regression, Ridge, SVR, Random Forest, Gradient Boosting, XGBoost) and two recurrent neural networks (LSTM, GRU) were trained, tuned, and compared on the same test set. Performance was measured with MAE, RMSE, MAPE, and R\textsuperscript{2}. The GRU model---using lag inputs at 1, 3, 6, 12, 24, and 48 hours---came out on top (RMSE 0.289, R\textsuperscript{2} 0.879, MAPE 7.36\%). LSTM followed closely (RMSE 0.314, R\textsuperscript{2} 0.857, MAPE 7.61\%), while tuned XGBoost led the ML pack (RMSE 0.384, R\textsuperscript{2} 0.831, MAPE 12.69\%). These error levels sit comfortably within the range utilities need for intraday market adjustments, showing that well-configured recurrent networks and boosted-tree ensembles can support real-world feeder-level forecasting and market participation.

\textbf{Keywords:} \textit{short-term load forecasting, machine learning, deep learning, GRU, LSTM, XGBoost, electricity markets, day-ahead market, intraday market, power distribution systems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% END OF ABSTRACT PAGE %%%%%%%%%%%%%%%

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% ACKNOWLEDGEMENT PAGE %%%%%%%%%%%%%%

\section*{\begin{center} {\sizexii ACKNOWLEDGEMENTS} \end{center}}
\phantomsection \addcontentsline{toc}{section}{Acknowledgements}\vspace{-16pt}
I would like to express my sincere gratitude to my supervisor and faculty members of the Department of Electrical Engineering for their valuable guidance, continuous support, and encouragement throughout the course of this project. Their technical insights and constructive feedback were instrumental in shaping this work.

I am also thankful to the Nepal Electricity Authority and relevant data-providing institutions for making the load and meteorological data available for this study. Their cooperation greatly contributed to the successful completion of the analysis.

Special thanks go to my friends and colleagues for their support, discussions, and motivation during the project period.

Finally, I would like to express my heartfelt appreciation to my family for their constant encouragement and support throughout my academic journey.

\vspace{24pt}
\hfill Sujit Koirala (PUL075MSPSE016)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% END OF ACKNOWLEDGEMENT PAGE %%%%%%%%%%%

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% TABLE OF CONTENTS %%%%%%%%%%%%%%%

\setlength{\cftbeforetoctitleskip}{-3em}
\renewcommand{\contentsname}{\centering \sizexii \underline{TABLE OF CONTENTS}}\label{TOC}
\begin{center}
\phantomsection  \addcontentsline{toc}{section}{Table of contents}
\tableofcontents 
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% END OF TABLE OF CONTENTS %%%%%%%%%%%%%

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% LIST OF TABLES %%%%%%%%%%%%%%%%

\setlength{\cftbeforelottitleskip}{-3em}
\setlength{\cfttabindent}{0pt}
\renewcommand{\listtablename}{\centering \sizexii LIST OF TABLES}\label{LOT}
\begin{center}
{
\let\oldnumberline\numberline
\renewcommand{\numberline}{\tablename~\oldnumberline}
\listoftables
}
\phantomsection  \addcontentsline{toc}{section}{List of tables}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% END OF LIST OF TABLES %%%%%%%%%%%%%%

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% LIST OF FIGURES %%%%%%%%%%%%%%%%

\setlength{\cftbeforeloftitleskip}{-3em}
\setlength{\cftfigindent}{0pt}
\renewcommand{\listfigurename}{\centering \sizexii LIST OF FIGURES}\label{LOF}
\begin{center} 
{
\let\oldnumberline\numberline
\renewcommand{\numberline}{\figurename~\oldnumberline}
\listoffigures 
}
\phantomsection \addcontentsline{toc}{section}{List of figures}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% END OF LIST OF FIGURES %%%%%%%%%%%%%%%

\clearpage



\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% LIST OF ACRONYMS AND ABBREVIATIONS %%%%%%%

\setlength{\topmargin}{-0.5in}
\section*{\begin{center}  {\sizexii LIST OF ACRONYMS AND ABBREVIATIONS} \end{center}}
\phantomsection  \addcontentsline{toc}{section}{List of acronyms and abbreviations}
\vspace{-16pt}
\begin{description}[font=\normalfont, leftmargin=45pt, labelwidth =\dimexpr75pt-\labelsep\relax]
\item[ANN]          :   \kern 1cm	Artificial Neural Network
\item[ARIMA]        :   \kern 1cm	Autoregressive Integrated Moving Average
\item[BS]           :   \kern 1cm	Bikram Sambat (Nepali Calendar)
\item[CNN]          :   \kern 1cm	Convolutional Neural Network
\item[DAM]          :   \kern 1cm	Day-Ahead Market
\item[DHM]          :   \kern 1cm	Department of Hydrology and Meteorology
\item[DL]           :   \kern 1cm	Deep Learning
\item[DWT]          :   \kern 1cm	Discrete Wavelet Transform
\item[GRU]          :   \kern 1cm	Gated Recurrent Unit
\item[IDM]          :   \kern 1cm	Intraday Market
\item[LSTM]         :   \kern 1cm	Long Short-Term Memory
\item[MAE]          :   \kern 1cm	Mean Absolute Error
\item[MAPE]         :   \kern 1cm	Mean Absolute Percentage Error
\item[ML]           :   \kern 1cm	Machine Learning
\item[MW]           :   \kern 1cm	Megawatt
\item[NEA]          :   \kern 1cm	Nepal Electricity Authority
\item[PCA]          :   \kern 1cm	Principal Component Analysis
\item[RF]           :   \kern 1cm	Random Forest
\item[RMSE]         :   \kern 1cm	Root Mean Squared Error
\item[RNN]          :   \kern 1cm	Recurrent Neural Network
\item[R\textsuperscript{2}]  :   \kern 1cm	Coefficient of Determination
\item[RTM]          :   \kern 1cm	Real-Time Market
\item[STLF]         :   \kern 1cm	Short-Term Load Forecasting
\item[SVR]          :   \kern 1cm	Support Vector Regression
\item[TCN]          :   \kern 1cm	Temporal Convolutional Network
\item[XGBoost]      :   \kern 1cm	Extreme Gradient Boosting
\end{description} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% END OF LIST OF ACRONYMS AND ABBREVIATIONS %%%%%

\clearpage
\pagenumbering{arabic}
%%%%%%%%%%% Roman page no before chapter 1 %%%%%%%%%%%
%\mainmatter
%\addtocontents{toc}{\protect\renewcommand{\protect\cftchappagefont}{\mdseries}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% Changing color of hyperlink to blue  %%%%%%%%%%%%%%%%%%%
\hypersetup{linkcolor = [rgb]{0.05098,0.50588,0.67451}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% CHAPTER ONE %%%%%%%%%%%%%%%%%%%%%

\chapter*{\vspace{-1in}\centering {\sizexii  CHAPTER ONE: INTRODUCTION}}
\addcontentsline{toc}{chapter}{\textbf{CHAPTER ONE: INTRODUCTION}}
\setcounter{chapter}{1}
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\vspace{-12pt}

\section{\sizexii Background}
This work looks at predicting electricity load one hour ahead for a single distribution feeder, relying on data-driven machine learning and deep learning methods. Hourly megawatt readings from the Baneshwor Feeder were combined with weather measurements and time markers to build and test several forecasting models. The goal was to find out which approach gives the most accurate and stable predictions---information that matters both for day-to-day grid operation and for trading energy in competitive markets.

Electricity use fluctuates constantly. People wake up and turn on lights, businesses open, air conditioners kick in during hot afternoons, and usage drops late at night. A system operator who can see even a few hours ahead can schedule generators more wisely, avoid expensive last-minute purchases, handle peak periods with less stress, and keep the supply steady \cite{aguilar2021short, chapagain2021short}. In energy markets, these forecasts let traders bid smarter and keep supply and demand in balance minute by minute \cite{weron2014electricity, kirschen2018fundamentals}.

\textbf{Why Short-Term Load Forecasting Matters:}
Good hour-ahead and day-ahead predictions help in several ways. On the \textit{operations} side, dispatchers can commit just the right mix of generators, plan maintenance windows without risking outages, and keep enough reserves on hand. On the \textit{money} side, forecast mistakes cost real cash---either from buying pricey peaking power at the last minute or from paying penalties for producing too much. For \textit{market players}, bids and offers hinge on expected demand; better forecasts mean better margins and fewer surprises in the day-ahead and intraday auctions \cite{weron2014electricity}. And as \textit{wind and solar} grow, knowing what the net load will be becomes essential for keeping the lights on and avoiding waste \cite{hong2016energy}.

Short-Term Load Forecasting (STLF) usually covers anything from one hour to roughly a day ahead. At these timescales, dispatchers decide which generators to run, how much reserve to hold, and how power flows through the network. For electricity markets, hour-ahead predictions feed into intraday trading sessions, letting participants tweak their positions as new information comes in, while very-short-term forecasts help real-time balancing and ancillary services \cite{lago2021dayahead, shahidehpour2002market}. Hour-ahead forecasting is especially handy because it gives utilities time to react to the latest weather changes and demand shifts. Traditionally, utilities leaned on simpler statistical tools---linear regression, ARIMA, exponential smoothing,
Holt-Winters \cite{acharya2021stlf, singla2019electrical}. These work fine for well-behaved patterns,
but they struggle when the load curve gets noisy, nonlinear, or tangled up with many
variables at once.

Machine learning models such as Random Forest, Support Vector Regression, and
XGBoost have proven useful for energy forecasting \cite{aguilar2021short}. They handle
nonlinear relationships better than classic statistics, which makes them a natural
choice for electricity load. Deep learning approaches---particularly recurrent networks
like LSTM and GRU---can pick up on time dependencies that simpler models miss
\cite{chapagain2021short}.

The Baneshwor Feeder of the Nepal Electricity Authority serves a mixed group of
consumers in the Baneshwor region of Kathmandu Valley, which is shown in single line
diagram shown in Figure 1.1. Its load pattern reflects residential lifestyles, commercial
activity, seasonal tourism impacts, and local weather changes. Daily and weekly cycles
are clearly visible, but there are also irregularities that simple models fail to capture.
As power consumption continues to grow and diversify, the ability to forecast the feeder’s short-term load accurately has become even more important \cite{singla2019electrical}. This creates	a strong motivation to investigate how modern ML and DL models can improve forecasting performance for this specific feeder.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{Figures/lineDiagram.png}
	\caption{Substation \& Transmission Line Network Baneshwor}
	\label{fig:line_diagram}
\end{figure}

\section{\sizexii Problem Statement}
Right now, forecasting for the Baneshwor Feeder mostly relies on rough guesses or
basic statistics. Those approaches do not capture how load behaves when temperature,
humidity, rain, weekends, and special events all come into play at once. Errors pile
up during peak hours, sudden weather shifts, and season changes. Although a few
studies have looked at load forecasting in Kathmandu valley, none have done a
head-to-head comparison of multiple ML and DL models with proper tuning, nor
framed the problem around electricity market needs.

Bad forecasts hurt both the grid and the wallet. Schedulers may hold too much
reserve (wasting money) or too little (risking shortages). Distribution losses can rise,
and in extreme cases voltage sags or load-shedding become necessary. In markets,
forecast errors show up as wrong bids, costly imbalance penalties, and inefficient
power purchases \cite{conejo2010decision}. With Nepal moving toward a more liberalized power
sector and cross-border trading, those costs will only grow. Hour-ahead predictions
are especially critical because utilities must adjust positions quickly as conditions
change \cite{zareipour2010electricity}.

Even though historical load and weather records exist, no one has yet run a proper
comparison of advanced ML and DL methods specifically for this feeder. Without such
a data-driven system, operators cannot tap into models that learn the complex
relationships hiding in the data.

This thesis fills that gap by building a complete forecasting pipeline---training
multiple ML and DL models, tuning their hyperparameters, comparing their accuracy,
and recommending the best option for hour-ahead prediction on the Baneshwor Feeder.



\section{\sizexii Objectives}
The main aim is to build and test ML and DL models for hour-ahead load forecasting
on the Baneshwor Feeder, improving prediction accuracy in a way that fits intraday
market timelines.
\begin{enumerate}
	\item Gather and clean historical load data, weather measurements, and calendar
	information for the feeder.
	\item Train and compare several ML models (SVR, Random Forest, XGBoost) and DL
	models (LSTM, GRU), measuring performance with RMSE, MAPE, MAE, and R\textsuperscript{2}.
	\item Produce forecasts that work for both day-ahead planning and hour-ahead market
	trading, helping with grid balancing and market participation.
	\item Recommend the best-performing model for real-world use on this feeder,
	balancing accuracy with practical market needs.
\end{enumerate}

\section{\sizexii Scope and Limitations}
This study covers only the Baneshwor Feeder under the Nepal Electricity Authority.
The time focus is short-term: predicting load one hour ahead using historical hourly
readings as the main input. That horizon matches what utilities need for intraday
market adjustments---the most fast-paced part of energy trading \cite{weron2014electricity}.
With hour-ahead forecasts, dispatchers can tweak positions, fine-tune dispatch, and
cut imbalance costs. The data set includes megawatt readings, temperature, humidity,
rainfall, plus calendar flags for weekdays, weekends, and holidays.

On the technical side, the study trains SVR, Random Forest, and XGBoost for
machine learning, and LSTM and GRU for deep learning. Model quality is judged by
RMSE, MAPE, MAE, and R\textsuperscript{2}---the same metrics used in electricity market
forecasting research, where accuracy drives how well a utility can adjust bids and
avoid imbalance penalties \cite{conejo2010decision}. Keep in mind that forecast quality depends
heavily on how complete and clean the historical records are. Also, this thesis sticks
to short-term horizons; medium- and long-term forecasting, renewable output
prediction, and electricity price forecasting are all outside its scope.

Even with solid results, several limitations came up during the work---mostly around
data, model assumptions, and scope:

\begin{enumerate}
	\item Data quality dependence: If the historical load or weather records have gaps, sensor glitches, or inconsistent entries, the models will suffer.
	\item Trouble with sudden events: Outages, festivals, freak weather, or unusual consumption spikes are hard for any data-driven model to predict.
	\item Heavy compute for deep learning: LSTM and GRU need more time and hardware than the ML models; results can vary with the setup used.
	\item Missing features: Economic activity, special events, industrial schedules, and price signals are not in the data, even though they could help.
	\item Feeder-specific models: The trained models fit the Baneshwor Feeder; applying them elsewhere would require retraining.
	\item Market scope: The forecasts target hour-ahead demand, not market bidding or price prediction; price-volume optimization is left for future work.
\end{enumerate}


\section{\sizexii Report Organization}
The thesis is laid out as follows. Chapter~1 explains what electrical load is, why
short-term forecasting matters for grid operation and energy markets, and introduces
the objectives, scope, and limitations. Chapter~2 reviews past studies, spots gaps,
and outlines the approach taken here; it also covers electricity market basics and the
math behind the ML/DL models. Chapter~3 walks through the workflow---data
collection, cleaning, model building, training, validation, tuning, and evaluation.
Chapter~4 shows exploratory analysis, model setups, hyperparameter grids, and a
side-by-side comparison of results relevant to operational planning and hour-ahead
trading. Chapter~5 wraps up with conclusions, limitations, market implications, and
ideas for future research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% END OF  CHAPTER ONE %%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% CHAPTER TWO %%%%%%%%%%%%%%%%%%%%%

\chapter*{\vspace{-1in}\centering {\sizexii  CHAPTER TWO: LITERATURE REVIEW}}
\addcontentsline{toc}{chapter}{\textbf{CHAPTER TWO: LITERATURE REVIEW}}
\setcounter{chapter}{2}
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\vspace{-12pt}

This chapter surveys what researchers have already done on short-term load
forecasting with ML and DL methods. It looks at the datasets, techniques, and
evaluation approaches used in power system forecasting, identifying trends,
strengths, and weak spots in the existing work. Spotting those gaps helps explain
why this study is needed. The review also sets the stage for the model choices and
methodology that follow.

\section{\sizexii Related Works}
A lot of research has tackled load forecasting---short-term, medium-term, and
long-term. Most papers focus on the short-term window.

\cite{singla2019electrical} employed Artificial Neural Networks for 24-hour short-term load forecasting, utilizing dew point temperature, dry bulb temperature, and humidity as input features. Their work demonstrated the effectiveness of ANN in capturing the relationship between weather variables and electrical load demand. Similarly, \cite{desai2021electrical} utilized the Prophet model from Meta to perform short-term load forecasting, incorporating time, temperature, humidity, and weather forecast data as features. \cite{matrenin2022medium} conducted a study on medium-term load forecasting using ensemble  machine learning models. They compared XGBoost and AdaBoost against traditional methods including SVR, decision trees, and Random Forest. Their results highlighted the superior performance of gradient boosting techniques for capturing complex load patterns. \cite{aguilar2021short} tested five machine learning models and found XGBoost to be the most accurate for predictions, using historical load data, weather information, and holiday indicators as input features. \cite{guo2021machine} analyzed three popular ML methods for load forecasting: Support Vector Machine, Random Forest, and LSTM. They proposed a fusion forecasting approach that combined outputs from all three models, demonstrating that ensemble methods could improve prediction accuracy beyond individual model performance. 
Different from above studies, \cite{saglam2024instantaneous} performed a comparison between optimization methods (Particle Swarm Optimization, Dandelion Optimizer, Growth Optimizer) and machine learning models (SVR, ANN) for instantaneous peak electrical load forecasting. They found that ANN combined with Growth Optimizer outperformed other models and identified a strong positive correlation between GDP and peak load demand. \cite{jain2024comparative} conducted a comprehensive evaluation of various machine learning algorithms for power load prediction, including Support Vector Machines, LSTM, ensemble classifiers, and Recurrent Neural Networks. They emphasized the importance of data preprocessing methods, feature selection strategies, and performance assessment metrics in achieving accurate forecasts, they demonstrated that ensemble methods and deep learning approaches consistently outperformed traditional
statistical models.

Deep learning is also widely used method for electrical load forecasting, \cite{chapagain2021short} explored deep learning models for electricity demand forecasting in Kathmandu Valley along with machine learning model. They found LSTM demonstrating outstanding performance in terms of MAPE and RMSE. \cite{acharya2021stlf} also performed short-term electrical load forecasting for the Gothatar feeder, uses six input features and found that Recurrent Neural Networks outperformed baseline methods including Single Exponential Smoothing, Double Exponential Smoothing, and Hot-Winter's method. \cite{cordeiro2023load} conducted a comprehensive comparison of load forecasting methods, including Random Forest, SVR, XGBoost, Multi-Layer Perceptron, LSTM, Conv-1D models and found that LSTM achieved the lowest error rates across multiple evaluation metrics. \cite{dong2024decade} provided a comprehensive survey on deep learning-based short-term electricity load forecasting covering the past decade. They identified CNN-LSTM hybrid architectures as widely adopted solutions due to exceptional performance in capturing both spatial and temporal features. 

Hybrid architecture are also widely adopted for time series forecasting. \cite{wen2024gru}  proposed a hybrid deep learning model combining Gated Recurrent Units and Temporal Convolutional Networks with an attention mechanism for short-term load forecasting. GRU captured long-term dependencies in time series data, while TCNefficiently learned patterns and features. Their approach demonstrated superior accuracy compared to standalone architectures. \cite{alhussein2020cnn} developed a hybrid CNN-LSTM framework for short-term individual household load forecasting, CNN layers for feature extraction from input data and LSTM layers for sequence
learning. This work demonstrated the effectiveness of combining convolutional and
recurrent architectures for handling high volatility in household-level load data. In
contrast, \cite{hasanat2024parallel} proposed a parallel multichannel network approach using 1D CNN and Bidirectional LSTM for load forecasting in smart grids. Unlike
traditional stacked CNN-LSTM architectures, their model independently processed
spatial and temporal characteristics through parallel channels.

\cite{vaswani2017attention} model are also widely used in time-series forecasting task. \cite{chan2024sparse} proposed a sparse transformer-based approach for electricity load forecasting that addressed the computational complexity limitations of standard transformer architectures, sparse attention mechanisms capture temporal dependencies more efficiently, achieving comparable accuracy to RNN-based state-of-the-art methods while being up to 5 times faster during inference. \cite{zhang2022time} developed a Time Augmented Transformer model for short-term electrical load forecasting, incorporating temporal features and self-attention mechanisms to capture complex dynamic non-linear sequence dependencies. Attention mechanism’s capacity to capture complex dynamical patterns in multivariate data contributed to improved forecasting accuracy. \cite{lu2024multivariate} proposed a multivariate data slicing transformer neural network for load forecasting in power systems. The transformer model excelled in capturing spatiotemporal relationships by self-attention mechanisms. Their approach demonstrated superior performance in handling the intermittency and volatility characteristics, outperforming traditional statistical models and conventional machine learning methods.

Ensemble methods are also applied in electrical load forecasting task. \cite{banik2024stacked} developed an enhanced stacked ensemble model combining Random Forest and XGBoost for renewable power and load forecasting, Random Forest first forecast the target variable, followed by XGBoost improving predictions through combination. 


\section{\sizexii Electricity Markets and Load Forecasting}

Over the past few decades, electricity markets have moved away from big monopolies
toward competitive setups where generation, transmission, and distribution are split
apart \cite{kirschen2018fundamentals}. In that market-driven world, load forecasting
is no longer just a technical chore---it directly affects trading efficiency, operating
costs, and the bottom line for everyone involved \cite{weron2014electricity}.

\subsection{\sizexii Structure of Electricity Markets}

Today's electricity markets run on several overlapping timeframes, each with its own
purpose \cite{shahidehpour2002market}. Knowing how these layers work shows why getting
the right forecast at the right horizon really matters:

\begin{enumerate}
    \item \textbf{Day-Ahead Market (DAM):} Traders buy and sell power for each hour of the next day, with bids typically due by noon. The market clears by matching supply and demand curves to set hourly prices and quantities. Day-ahead forecasts (24--48 hours out) drive bidding, scheduling, and unit-commitment choices. If those forecasts are off, participants either overpay later or sell at a loss \cite{conejo2010decision}.
    
    \item \textbf{Intraday Market (IDM):} This market lets participants trade power for delivery the same day, often until an hour or even 15 minutes before real-time. As new weather or demand data comes in, traders adjust their positions. Hour-ahead forecasts are key here---they help utilities cut imbalance exposure and react to surprises that the day-ahead forecast missed. Accurate hour-ahead numbers make or break intraday strategies and directly affect how much a utility pays in imbalance fees \cite{zareipour2010electricity}.
        
    \item \textbf{Real-Time Balancing Market (RTM):} This market runs continuously to keep generation and load in balance second by second. Operators buy frequency regulation and reserves, and settle any leftover imbalances here. Very-short-term forecasts (minutes to an hour) keep things running smoothly and dodge expensive emergency actions. Imbalance prices can spike high---or go negative---creating a strong incentive to forecast well \cite{hobbs2001evaluation}.
\end{enumerate}

Figure~\ref{fig:market_timeline} illustrates the relationship between forecasting horizons and electricity market operational timelines.

\begin{figure}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Market} & \textbf{Trading Horizon} & \textbf{Forecast Need} & \textbf{Key Use} \\
    \hline
    Day-Ahead & D-1 (noon) & 24--48 hours & Unit commitment, bidding \\
    \hline
    Intraday & Same day & 1--12 hours & Position adjustment \\
    \hline
    Real-Time & Continuous & Minutes--1 hour & Balancing, reserves \\
    \hline
    \end{tabular}
    \caption{Electricity Market Timeline and Forecasting Requirements}
    \label{fig:market_timeline}
\end{figure}

\subsection{\sizexii Role of Load Forecasting in Market Operations}

Load forecasts serve several practical purposes in market operations \cite{hong2016energy, conejo2010decision}:

\textbf{Day-Ahead Bidding:} Most trading happens the day before delivery. Utilities submit demand estimates by noon, and those numbers shape bids, generation schedules, and commitment plans. Underestimate load and you buy expensive power at the last minute; overestimate and you dump the extra at a loss. For big utilities running gigawatt portfolios, even small errors add up fast \cite{conejo2010decision}.

\textbf{Intraday Adjustments:} As delivery nears, fresher forecasts let traders tweak positions. Day-ahead guesses always have some error---weather changes, random demand bumps---so hour-ahead predictions let utilities close the gap between what they contracted and what they actually need. Good hour-ahead numbers cut dependence on pricey real-time fixes.

\textbf{Imbalance Cost Control:} Any mismatch between forecasted and actual load ends up settled in the real-time market, often at unfavorable rates. During tight supply, imbalance prices can spike several times higher than day-ahead levels---or even go negative when there is too much power. Accurate hour-ahead forecasts catch the latest demand and weather shifts, giving utilities time to correct through intraday trades and dodge steep imbalance bills \cite{zareipour2010electricity}.

\textbf{Grid Reliability:} System operators rely on load forecasts to set reserves, manage congestion, and keep frequency stable. Better feeder-level predictions roll up into better system-wide visibility.

\textbf{Economic Dispatch:} Dispatch and scheduling algorithms eat load forecasts as their main input. Tighter forecasts mean leaner reserves, lower fuel bills, and better use of generation assets.

\subsection{\sizexii Relevance to Nepal's Power Sector}

Nepal still runs a centralized utility model under the Nepal Electricity Authority
(NEA), but change is underway. Cross-border trading with India, rising private
generation, and shifting regulations point toward more market-like operations down
the road. Building solid feeder-level forecasting now makes sense for:

\begin{itemize}
    \item Scheduling imports and power-purchase agreements better
    \item Getting ready for a possible wholesale market
    \item Running demand-side programs
    \item Plugging in distributed solar and batteries smoothly
    \item Cutting technical and commercial losses through smarter operations
\end{itemize}

The hour-ahead and day-ahead forecasts built in this thesis match those market
timescales, so the framework can serve today's grid management and tomorrow's
market participation.


\section{\sizexii Theoretical Background of Forecasting Models}

This section explains the math and inner workings of the models used in this study.
Understanding how each algorithm learns patterns helps make sense of the forecasting
results later on.

\subsection{\sizexii Machine Learning Models}

\subsubsection{Linear Regression}

Linear Regression is the simplest starting point---it assumes a straight-line relationship
between inputs and output. Electricity load is not really linear, but this model gives a
useful baseline to compare fancier methods against.

\textbf{Mathematical Formulation:}
\begin{equation}
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n
\end{equation}
where $\beta_0$ is the intercept (bias term), $\beta_i$ are the coefficients (weights) learned via ordinary least squares minimization, and $x_i$ are the input features.

\subsubsection{Ridge Regression}

Ridge Regression \cite{hoerl1970ridge} adds L2 regularization to the linear model to reduce overfitting and stabilize coefficient estimates. It is more robust than standard Linear Regression when dealing with many correlated features, which is the case in this study.

\textbf{Mathematical Formulation:}
\begin{equation}
\min_{\beta} \left( \| y - X\beta \|^2 + \alpha \| \beta \|^2 \right)
\end{equation}
where $\alpha$ controls the strength of L2 regularization, $\| y - X\beta \|^2$ is the residual sum of squares, and $\| \beta \|^2$ is the L2 norm of the coefficient vector.

\subsubsection{Support Vector Regression (SVR)}

SVR \cite{drucker1997svr} models nonlinear relationships by mapping features into a high-dimensional space using kernel functions. It works well for complex regression problems with moderate dataset sizes.

\textbf{Mathematical Formulation:}

SVR finds a function $f(x)$ by solving the following optimization problem:
\begin{equation}
\min_{w, b, \xi, \xi^*} \left( \frac{1}{2} \| w \|^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*) \right)
\end{equation}
subject to the constraints:
\begin{align}
y_i - (w \cdot x_i + b) &\leq \varepsilon + \xi_i \nonumber\\
(w \cdot x_i + b) - y_i &\leq \varepsilon + \xi_i^* \nonumber\\
\xi_i, \xi_i^* &\geq 0 \nonumber
\end{align}
where $w$ is the weight vector, $b$ is the bias term, $C$ is the regularization parameter controlling the trade-off between flatness and tolerance of deviations, $\varepsilon$ defines the epsilon-insensitive tube, and $\xi_i$ and $\xi_i^*$ are slack variables for points outside the tube.

\subsubsection{Random Forest Regressor}

Random Forest \cite{breiman2001random} is an ensemble method consisting of multiple decision trees. Each tree is trained on a random subset of features and samples (bootstrap aggregating). It is robust, stable, and handles nonlinearity effectively.

\textbf{Mathematical Formulation:}

The Random Forest prediction is the average of all individual tree predictions:
\begin{equation}
\hat{y} = \frac{1}{T} \sum_{t=1}^{T} f_t(x)
\end{equation}
where $T$ is the total number of trees in the forest and $f_t(x)$ is the prediction of the $t$-th decision tree.

\subsubsection{Gradient Boosting Regressor}

Gradient Boosting \cite{friedman2001gradient} builds trees sequentially, with each new tree correcting the errors (residuals) of the previous ensemble. It is particularly effective for structured tabular data like load forecasting.

\textbf{Mathematical Formulation:}

At each boosting iteration $m$, the model is updated as:
\begin{equation}
F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
\end{equation}
where $F_{m-1}(x)$ is the ensemble prediction from the previous iteration, $h_m(x)$ is the new tree fitted to the negative gradient (pseudo-residuals), and $\nu$ is the learning rate (shrinkage factor) that controls the contribution of each tree.

\subsubsection{XGBoost Regressor}

XGBoost (Extreme Gradient Boosting) \cite{chen2016xgboost} is an optimized and regularized implementation of gradient boosting designed for efficiency, scalability, and high accuracy. It was one of the best-performing ML models in this study.

\textbf{Mathematical Formulation:}

XGBoost minimizes the following regularized objective function:
\begin{equation}
\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
\end{equation}
where $l(y_i, \hat{y}_i)$ is the loss function measuring the difference between actual and predicted values, and $\Omega(f_k)$ is the regularization term for the $k$-th tree, defined as:
\begin{equation}
\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
\end{equation}
where $T$ is the number of leaves in the tree, $w_j$ is the weight (score) of the $j$-th leaf, $\gamma$ controls the minimum loss reduction required to make a split, and $\lambda$ is the L2 regularization term on leaf weights.

\subsection{\sizexii Deep Learning Models}

To capture the nonlinear patterns and time dependencies in feeder load data, two
recurrent neural network types were built and tested: LSTM and GRU. Both were
trained on sliding windows of consecutive hourly readings, letting them pick up
both short-term and longer-term patterns. Training used the Adam optimizer
\cite{kingma2014adam}, with early stopping to avoid overfitting.

\subsubsection{Long Short-Term Memory (LSTM)}

LSTM networks \cite{hochreiter1997lstm} keep a running memory over long sequences using
gates that control what to remember and what to forget. That makes them a natural
choice for load forecasting, where today's demand depends on what happened hours or
days ago. Unlike plain RNNs, LSTMs do not suffer from vanishing gradients.

\textbf{Mathematical Formulation:}

At each time step $t$, the LSTM computes the following:

Forget gate (determines what information to discard from cell state):
\begin{equation}
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\end{equation}

Input gate (determines what new information to store):
\begin{equation}
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\end{equation}

Candidate cell state (creates new candidate values):
\begin{equation}
\tilde{c}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
\end{equation}

Cell state update (combines old and new information):
\begin{equation}
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
\end{equation}

Output gate (determines what to output):
\begin{equation}
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\end{equation}

Hidden state (final output at time step $t$):
\begin{equation}
h_t = o_t \odot \tanh(c_t)
\end{equation}
where $\sigma$ is the sigmoid activation function, $\tanh$ is the hyperbolic tangent activation function, $\odot$ denotes element-wise (Hadamard) product, $[h_{t-1}, x_t]$ represents concatenation of the previous hidden state and current input, $W_f$, $W_i$, $W_c$, and $W_o$ are weight matrices for each gate, $b_f$, $b_i$, $b_c$, and $b_o$ are bias vectors for each gate, $c_t$ is the cell state that carries long-term memory, and $h_t$ is the hidden state output.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/lstm.png}
    \caption{Architecture LSTM}
    \label{fig:lstm_architecture}
\end{figure}

\subsubsection{Gated Recurrent Unit (GRU)}

GRU \cite{cho2014gru} is a slimmer cousin of LSTM. It merges the forget and input
gates into one update gate and combines the cell state with the hidden state. The
result is fewer parameters, faster training, and often similar accuracy.

\textbf{Mathematical Formulation:}

At each time step $t$, the GRU computes the following:

Update gate (controls how much past information to keep):
\begin{equation}
z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
\end{equation}

Reset gate (determines how much past information to forget):
\begin{equation}
r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
\end{equation}

Candidate hidden state (computes new candidate activation):
\begin{equation}
\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)
\end{equation}

Final hidden state (interpolates between previous and candidate state):
\begin{equation}
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{equation}
where $\sigma$ is the sigmoid activation function, $\tanh$ is the hyperbolic tangent activation function, $\odot$ denotes element-wise (Hadamard) product, $[h_{t-1}, x_t]$ represents concatenation of the previous hidden state and current input, $W_z$, $W_r$, and $W_h$ are weight matrices for the update gate, reset gate, and candidate state, $b_z$, $b_r$, and $b_h$ are bias vectors, $z_t$ controls the balance between old and new information, and $r_t$ controls how much of the previous state influences the candidate.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/gru.png}
    \caption{Architecture GRU}
    \label{fig:gru_architecture}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% END OF CHAPTER TWO %%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% CHAPTER THREE %%%%%%%%%%%%%%%%%%%%%

\chapter*{\vspace{-1in}\centering {\sizexii CHAPTER THREE: RESEARCH METHODOLOGY}}
\addcontentsline{toc}{chapter}{\textbf{CHAPTER THREE: RESEARCH METHODOLOGY}}
\setcounter{chapter}{3}
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\vspace{-12pt}

This chapter explains how the study was carried out---from gathering data to cleaning
it, building models, and measuring their accuracy. The workflow is set up for
hour-ahead forecasting, which fits the timescales utilities need for intraday market
adjustments. A block diagram summarizes the steps, and the rest of the chapter digs
into each part in detail.

\section{\sizexii Overall Workflow}

The pipeline follows a step-by-step path aimed at producing hour-ahead load
forecasts. Raw hourly megawatt readings from the Baneshwor Feeder get combined
with weather data (temperature, humidity, rainfall) and calendar flags
(weekday/weekend, holidays). The hourly granularity matches the settlement periods
used in intraday markets \cite{weron2014electricity}.

First, the raw records go through quality checks---filling gaps, flagging outliers,
fixing timestamps, and building new features like sine-cosine encodings for hour and
month. Then an exploratory analysis looks at daily, weekly, and seasonal patterns,
plus how weather and load are linked, to pick the most useful inputs.

Next, both ML models (fed a tabular feature matrix) and recurrent DL models (fed
sequence windows) are trained on the cleaned data. Hyperparameters are tuned via
GridSearchCV for ML and iterative experiments for LSTM/GRU. Finally, all models
are compared using RMSE, MAE, MAPE, and R\textsuperscript{2}---standard metrics in
market forecasting research \cite{hong2016energy}---and the best option is recommended
for operational use. Figure~\ref{fig:block_diagram} shows the full workflow.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/blockDiagram.png}
    \caption{Methodology Block Diagram}
    \label{fig:block_diagram}
\end{figure}

\section{\sizexii Data Acquisition}

The first step was gathering the data. Hourly load readings came from archived logs
at the Baneshwor substation. Weather data---temperature, humidity, and solar
radiation---came from Nepal's Department of Hydrology and Meteorology. Calendar
info (weekdays, weekends) was pulled from official government sources.

Two main data streams feed the models:
\begin{enumerate}
    \item Hourly MW readings from the Baneshwor Feeder
    \item Hourly weather records (temperature, humidity, solar radiation)
\end{enumerate}

Both arrived as messy Excel files---different formats, inconsistent timestamps, gaps,
multiple sheets per month---so a custom multi-step cleaning pipeline was needed.
Details on each source follow.

\subsection{\sizexii Data Sources}

\begin{enumerate}
    \item[a)] \textbf{Electrical Load Data:} The electrical load data used in this study was obtained from the Baneshwor Substation, which operates under the Nepal Electricity Authority (NEA). Hourly feeder load readings were collected from archived operational log sheets maintained by the substation for the years 2079 to 2082 BS. These records provided raw POWER (MW) measurements for the Baneshwor Feeder, along with associated timestamp information. Since the data originated from manually recorded and distributed Excel files, several preprocessing steps---such as header correction, timestamp standardization, and quality checks---were required before the dataset could be used for modeling. This substation-provided dataset forms the core of the forecasting analysis, representing real operational feeder behavior across multiple years.
    
    \item[b)] \textbf{Weather Data:} Weather data was sourced from Nepal's Department of Hydrology and Meteorology (DHM), the official governmental agency responsible for climate and atmospheric measurements. The dataset included hourly records of air temperature, relative humidity, and global solar radiation for the corresponding study period. These variables were essential for capturing the environmental conditions influencing electricity consumption patterns. The DHM dataset required timestamp alignment, interpolation for missing values, and smoothing of extreme readings to ensure compatibility with the load dataset. Once cleaned and synchronized, the weather data served as an important set of exogenous features for both machine learning and deep learning models.
\end{enumerate}

Because the raw files came in varying formats---different months, unpredictable sheet names, Bikram Sambat (BS) dates, mixed day formats, half-hour readings, inconsistent header rows, and multiple sheets per month---a custom data acquisition pipeline was required.

\subsection{\sizexii Load Data Acquisition and Structuring Process}

The original Excel files provided by the Nepal Electricity Authority (NEA) were highly heterogeneous in structure. Each month consisted of multiple workbooks, with each workbook containing several sheets. In many cases, sheets included mixed headers, irrelevant rows, inconsistent timestamp formats, and non-uniform naming conventions. To address these issues and convert the raw data into a single unified structure suitable for analysis, a multi-stage data processing pipeline was implemented.

In the first stage, a month-wise sheet extraction process was carried out. The script automatically scanned all monthly datasets and identified Excel sheets whose names contained ``11KV'' or its variations. The extracted sheets were then aligned with their corresponding time periods to ensure correct temporal ordering. Only rows with timestamps recorded at exact hourly intervals (HH:00) were retained, while half-hour readings such as 7:30 were intentionally excluded to maintain uniform hourly resolution. The valid hourly records from each sheet were compiled to produce clean month-wise datasets. At this stage, although the data were organized chronologically, the timestamps were still recorded in the Nepali calendar (BS) and exhibited format inconsistencies.

The second stage focused on date conversion, hour normalization, and daily data structuring. The BS date embedded within each sheet name was extracted and converted into the Gregorian (AD) calendar using Nepali date conversion libraries. Each sheet was read without assuming a fixed header position, allowing the script to dynamically identify the Time column and the corresponding POWER (MW) values. Every day was standardized to contain exactly twenty-four hourly records by indexing hours from 1 to 24, and any missing hours were filled using linear interpolation. Clean and consistent timestamps were then generated in the standard hourly format, ensuring temporal continuity across the dataset. This process resulted in one clean and complete daily record for each calendar day.

In the final stage, all structured monthly and yearly datasets were merged into a single consolidated file. The script systematically extracted the valid Time and POWER (MW) columns, removed any remaining header fragments, and concatenated the data in chronological order. This process produced a fully unified dataset containing continuous hourly POWER (MW) measurements for the entire study period, which served as the foundation for subsequent data analysis and load forecasting model development.

\subsection{\sizexii Weather Data Acquisition and Structuring}

Weather data was also provided in raw format with mixed timestamps. Two scripts were developed to clean and align it with the load data.

\begin{enumerate}
    \item[a)] \textbf{Extracting and Cleaning Raw Weather File:} The script located the correct columns for time, temperature, humidity, and solar radiation, then removed any unusable rows. All timestamps were parsed into a consistent datetime format, after which the weather data was filtered to match the exact date range of the load dataset. The timestamps were then formatted as YYYY-MM-DD HH:MM. This stage produced a clean hourly weather dataset.
    
    \item[b)] \textbf{Structuring Weather Timestamp Alignment:} Timestamps were shifted so that values such as ``HH:45'' were aligned to the next hour at ``HH+1:00,'' and all ``24:00'' rollover cases were handled correctly. Missing or zero weather values were replaced using nearest-neighbor averages, while NaN solar radiation entries were set to zero. These steps produced the final clean weather file and ensured that all weather variables followed the exact hourly structure required for forecasting.
\end{enumerate}

\subsection{\sizexii Final Merging of Load and Weather Data}

In the final stage, load and weather datasets were merged into a single unified file. All load timestamps were carefully parsed, including proper handling of the special \texttt{24:00} time format, while weather timestamps were standardized to a consistent format. Weather observations were then precisely aligned with their corresponding load timestamps, and any missing values in weather variables were filled using linear interpolation. The resulting dataset contained synchronized records of time, electrical load (MW), air temperature, global solar radiation, and relative humidity, and served as the primary input for all machine learning and deep learning models used in this thesis.

\section{\sizexii Data Preprocessing}

Once load and weather data were merged into one hourly file, more cleaning was
needed before modeling. Timestamps came in mixed formats---some even listed
``24:00''---so a custom routine shifted those to 00:00 of the next day and
standardized everything. Missing MW values (from incomplete logs or bad readings)
were filled with forward-fill then backward-fill to keep the time series smooth.
Weather gaps got linear interpolation; solar radiation was set to zero at night, and
wild humidity or temperature spikes were smoothed using nearby values. The result
was a clean, gap-free, time-aligned dataset.

Next came feature engineering to capture daily, weekly, and seasonal rhythms. From
each timestamp, the pipeline extracted hour, day, month, day-of-week, week-of-year,
and a weekend flag. Because time is cyclical---hour 23 wraps to hour 0---sine and
cosine transforms were applied to hour, month, and day-of-week. These encodings
let the models learn smooth periodic patterns instead of seeing artificial jumps at
midnight or January 1. The final feature set combined weather variables with these
temporal components.

A correlation check (Figure~\ref{fig:feature_correlation}) showed which features matter
most. Hour of day has the strongest link to load; solar radiation shows a moderate
positive tie; temperature and humidity contribute weaker but still useful signals;
calendar variables add subtle seasonal cues. All engineered features were kept based
on this analysis and domain knowledge. After preprocessing, the dataset had no
missing values, no weird timestamps, and no half-hour entries---ready for modeling.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/featureCorrelation.png}
    \caption{Feature Correlation Matrix}
    \label{fig:feature_correlation}
\end{figure}

\section{\sizexii Data Input Structure}

This section describes the structure and format of the input data used for training the machine learning and deep learning models. Understanding the data input structure is essential for reproducibility and for applying similar methodologies to other forecasting problems.

\subsection{\sizexii Raw Data Format}

The final merged dataset consists of hourly records with the following structure:

\begin{table}[H]
\centering
\caption{Raw Input Data Format}
\label{tab:raw_data_format}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Column} & \textbf{Data Type} & \textbf{Unit} & \textbf{Description} \\
\hline
Time & Datetime & YYYY-MM-DD HH:MM & Hourly timestamp \\
\hline
MW & Float & Megawatt (MW) & Electrical load (target variable) \\
\hline
Air Temperature & Float & \textdegree C & Ambient temperature \\
\hline
Global Solar Radiation & Float & W/m\textsuperscript{2} & Solar irradiance \\
\hline
Relative Humidity & Float & \% & Atmospheric humidity \\
\hline
\end{tabular}
\end{table}

Table~\ref{tab:sample_data} shows a sample of the cleaned dataset used in this study:

\begin{table}[H]
\centering
\caption{Sample Data Records from Cleaned Dataset}
\label{tab:sample_data}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Time} & \textbf{MW} & \textbf{Air Temp (\textdegree C)} & \textbf{Solar Rad (W/m\textsuperscript{2})} & \textbf{RH (\%)} \\
\hline
2022-10-19 01:00 & 0.8 & 14.5 & 0.0 & 88.8 \\
2022-10-19 02:00 & 0.8 & 14.4 & 0.0 & 87.9 \\
2022-10-19 06:00 & 1.2 & 12.1 & 0.0 & 100.0 \\
2022-10-19 12:00 & 1.8 & 22.0 & 822.0 & 46.5 \\
2022-10-19 19:00 & 2.8 & 17.9 & 0.0 & 76.7 \\
\hline
\end{tabular}
\end{table}

\subsection{\sizexii Feature Engineering and Input Vector}

After preprocessing and feature engineering, each input sample contains the following features that are fed into the model nodes:

\begin{table}[H]
\centering
\caption{Engineered Feature Set for Model Input}
\label{tab:feature_set}
\begin{tabular}{|c|l|l|l|}
\hline
\textbf{No.} & \textbf{Feature} & \textbf{Type} & \textbf{Description} \\
\hline
1 & Hour & Integer (0--23) & Hour of day \\
2 & Day & Integer (1--31) & Day of month \\
3 & Month & Integer (1--12) & Month of year \\
4 & Day\_of\_Week & Integer (0--6) & Day of week (Mon=0) \\
5 & Week\_of\_Year & Integer (1--52) & Week number \\
6 & Is\_Weekend & Binary (0/1) & Weekend indicator \\
7 & Hour\_Sin & Float (-1 to 1) & $\sin(2\pi \cdot \text{Hour}/24)$ \\
8 & Hour\_Cos & Float (-1 to 1) & $\cos(2\pi \cdot \text{Hour}/24)$ \\
9 & Month\_Sin & Float (-1 to 1) & $\sin(2\pi \cdot \text{Month}/12)$ \\
10 & Month\_Cos & Float (-1 to 1) & $\cos(2\pi \cdot \text{Month}/12)$ \\
11 & DoW\_Sin & Float (-1 to 1) & $\sin(2\pi \cdot \text{DoW}/7)$ \\
12 & DoW\_Cos & Float (-1 to 1) & $\cos(2\pi \cdot \text{DoW}/7)$ \\
13 & Air\_Temperature & Float & Scaled temperature \\
14 & Global\_Solar\_Radiation & Float & Scaled solar radiation \\
15 & Relative\_Humidity & Float & Scaled humidity \\
\hline
\end{tabular}
\end{table}

\subsection{\sizexii Input Structure for Machine Learning Models}

For machine learning models (Linear Regression, Ridge, SVR, Random Forest, Gradient Boosting, XGBoost), the input is structured as a 2D feature matrix:

\begin{equation}
X_{ML} = \begin{bmatrix}
x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}
\end{equation}

where $m$ is the number of samples and $n$ is the number of features. The target vector is:

\begin{equation}
y = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix}
\end{equation}

where $y^{(i)}$ represents the load value (MW) at the next hour that the model predicts.

\subsection{\sizexii Input Structure for Deep Learning Models (LSTM/GRU)}

For recurrent neural networks (LSTM and GRU), the input is structured as a 3D tensor to capture temporal sequences:

\begin{equation}
X_{DL} \in \mathbb{R}^{m \times T \times n}
\end{equation}

where $m$ is the number of samples, $T$ is the sequence length (lookback window, typically 24 hours), and $n$ is the number of features per timestep.

Figure~\ref{fig:lstm_input} illustrates how the sliding window approach creates input sequences for the LSTM/GRU models:

\begin{figure}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    & \multicolumn{4}{c|}{\textbf{Input Sequence (T=24 hours)}} & \textbf{Target} \\
    \hline
    Sample & $t-24$ & $t-23$ & $\cdots$ & $t-1$ & $t$ \\
    \hline
    1 & $\mathbf{x}_{1}$ & $\mathbf{x}_{2}$ & $\cdots$ & $\mathbf{x}_{24}$ & $y_{25}$ \\
    2 & $\mathbf{x}_{2}$ & $\mathbf{x}_{3}$ & $\cdots$ & $\mathbf{x}_{25}$ & $y_{26}$ \\
    3 & $\mathbf{x}_{3}$ & $\mathbf{x}_{4}$ & $\cdots$ & $\mathbf{x}_{26}$ & $y_{27}$ \\
    $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ \\
    \hline
    \end{tabular}
    \caption{Sliding Window Sequence Construction for LSTM/GRU Input}
    \label{fig:lstm_input}
\end{figure}

Each $\mathbf{x}_t$ represents the feature vector at timestep $t$:
\begin{equation}
\mathbf{x}_t = [\text{Hour}_t, \text{Hour\_Sin}_t, \text{Hour\_Cos}_t, \ldots, \text{Lag}_1, \text{Lag}_3, \ldots, \text{Lag}_{48}]
\end{equation}

\subsection{\sizexii Lag Feature Configuration for Deep Learning}

To enhance temporal pattern recognition, lag features are added to the input vector. Three configurations were tested:

\begin{table}[H]
\centering
\caption{Lag Feature Configurations for Deep Learning Models}
\label{tab:lag_config}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Configuration} & \textbf{Lag Hours} & \textbf{Purpose} \\
\hline
Short & [1, 3, 6] & Recent hour patterns \\
\hline
Medium & [1, 3, 6, 12, 24] & Daily patterns \\
\hline
Long (Best) & [1, 3, 6, 12, 24, 48] & Multi-day dependencies \\
\hline
\end{tabular}
\end{table}

For example, with the long lag configuration, the lag features for predicting load at time $t$ include:
\begin{itemize}
    \item $\text{Lag}_1$: Load at $t-1$ (1 hour ago)
    \item $\text{Lag}_3$: Load at $t-3$ (3 hours ago)
    \item $\text{Lag}_6$: Load at $t-6$ (6 hours ago)
    \item $\text{Lag}_{12}$: Load at $t-12$ (12 hours ago)
    \item $\text{Lag}_{24}$: Load at $t-24$ (same hour yesterday)
    \item $\text{Lag}_{48}$: Load at $t-48$ (same hour two days ago)
\end{itemize}

\subsection{\sizexii Complete Input Pipeline Summary}

Figure~\ref{fig:data_pipeline} summarizes the complete data input pipeline from raw data to model nodes:

\begin{figure}[H]
    \centering
    \fbox{\parbox{0.9\textwidth}{
    \centering
    \textbf{Data Input Pipeline}
    \vspace{6pt}
    
    \textbf{Step 1: Raw Data} $\rightarrow$ Time, MW, Weather Variables
    
    $\downarrow$
    
    \textbf{Step 2: Preprocessing} $\rightarrow$ Cleaning, Outlier Removal, Missing Value Imputation
    
    $\downarrow$
    
    \textbf{Step 3: Feature Engineering} $\rightarrow$ Temporal Features, Cyclical Encoding, Lag Features
    
    $\downarrow$
    
    \textbf{Step 4: Scaling} $\rightarrow$ StandardScaler (zero mean, unit variance)
    
    $\downarrow$
    
    \textbf{Step 5a: ML Input} $\rightarrow$ 2D Matrix $[m \times n]$ $\rightarrow$ ML Model Nodes
    
    \textbf{Step 5b: DL Input} $\rightarrow$ 3D Tensor $[m \times T \times n]$ $\rightarrow$ LSTM/GRU Nodes
    
    $\downarrow$
    
    \textbf{Output:} Predicted Load (MW) for next hour
    }}
    \caption{Complete Data Input Pipeline from Raw Data to Model Nodes}
    \label{fig:data_pipeline}
\end{figure}

This structured input representation ensures that both machine learning and deep learning models receive appropriately formatted data that captures temporal patterns, weather influences, and historical load dependencies necessary for accurate hour-ahead load forecasting.

\section{\sizexii Model Development}

Based on the theoretical foundations presented in Chapter 2, this study implemented multiple forecasting models to predict short-term electrical load for the Baneshwor Feeder. For machine learning, six models were developed: Linear Regression, Ridge Regression, Support Vector Regression (SVR), Random Forest Regressor, Gradient Boosting Regressor, and XGBoost Regressor. For deep learning, two recurrent architectures were implemented: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). All models were trained on the cleaned and feature-engineered dataset described in the earlier sections, with each model receiving the same input feature set to ensure fair comparison. The machine learning pipeline involved standard scaling of the numerical features, an 80--20 train--test split, and hyperparameter tuning performed using GridSearchCV, while the deep learning models were trained on sliding windows of historical hourly sequences using the Adam optimizer with early stopping to prevent overfitting. Model performance was evaluated using RMSE, MAE, MAPE, and R².

\section{\sizexii Model Training and Validation}

This stage focuses on how both machine learning (ML) and deep learning (DL) models were trained, tuned, validated, and prepared for final performance comparison. Since ML and DL models require different handling, the training process is presented in two separate parts.

\subsection{\sizexii Training of Machine Learning Models}

The machine learning models trained in this study include:
\begin{enumerate}
    \item Support Vector Regression (SVR)
    \item Random Forest Regressor (RF)
    \item Gradient Boosting Regressor (GBR)
    \item Extreme Gradient Boosting (XGBoost)
    \item Linear Regression and Ridge Regression (baseline models)
\end{enumerate}

All models used the same final preprocessed dataset described earlier, containing weather features, temporal encodings, and cleaned load values.

\subsubsection{Input Preparation}

For ML models, the dataset was used in a tabular format:
\begin{enumerate}
    \item \textbf{Features (X):} Time features (hour, month, weekday), cyclical encodings, weather variables, and lag features if applied.
    \item \textbf{Target (y):} Load at the next hour.
\end{enumerate}

Since ML models do not operate on sequences, no sliding window was required.

\subsubsection{Data Splitting}

The dataset was split into 80 percent for training and 20 percent for testing. Shuffling was applied during the split to prevent temporal clustering and to ensure that the machine learning models were exposed to a diverse mix of seasonal and temporal patterns during training.

\subsubsection{Feature Scaling}

Some models required normalized inputs, so StandardScaler was applied to Linear Regression, Ridge, and SVR. Tree-based models such as Random Forest, Gradient Boosting, and XGBoost did not require any scaling.

\subsubsection{Training Procedure}

Each model was trained using its corresponding optimization approach:
\begin{itemize}
    \item Least squares optimization for Linear Regression and Ridge
    \item Kernel-based optimization for SVR
    \item Ensemble tree learning for Random Forest and Gradient Boosting
    \item Gradient-boosted tree optimization for XGBoost
\end{itemize}

The training process involved fitting the models on the training set, generating predictions on the test set, and evaluating performance using RMSE, MAE, MAPE, and R².

\subsubsection{Hyperparameter Tuning}

All machine learning models were fine-tuned using GridSearchCV, which tested different combinations of hyperparameters:
\begin{itemize}
    \item \textbf{SVR:} C, epsilon, and gamma
    \item \textbf{Random Forest and Gradient Boosting:} n\_estimators, max\_depth, and min\_samples\_split
    \item \textbf{XGBoost:} learning\_rate, max\_depth, subsample, and colsample\_bytree
    \item \textbf{Ridge:} alpha
\end{itemize}

The best configurations were selected based on the lowest validation error.

\subsection{\sizexii Training of Deep Learning Models}

Two deep learning models were implemented:
\begin{enumerate}
    \item Long Short-Term Memory (LSTM)
    \item Gated Recurrent Unit (GRU)
\end{enumerate}

Since deep learning models learn from sequences rather than static features, the training process follows a different pipeline.

\subsubsection{Sequence Construction}

A sliding window method was used in which the model received the past $N$ hours as input and predicted the load for the next hour. A typical window size of 24 hours was used, although this value can be adjusted in the implementation.

\subsubsection{Train--Validation--Test Split}

Deep learning models require sequential integrity, so the dataset was split chronologically:
\begin{itemize}
    \item 70 percent for training
    \item 15 percent for validation
    \item 15 percent for testing
\end{itemize}

No shuffling was applied, ensuring that the model learned from the natural temporal progression of the data.

\subsubsection{Lag Feature Configurations}

To capture temporal dependencies at multiple scales, three different lag configurations were evaluated:
\begin{itemize}
    \item \textbf{Short:} Lags at [1, 3, 6] hours
    \item \textbf{Medium:} Lags at [1, 3, 6, 12, 24] hours
    \item \textbf{Long:} Lags at [1, 3, 6, 12, 24, 48] hours
\end{itemize}

The extended lag configuration proved most effective for the recurrent models by providing explicit historical context at various temporal resolutions.

\subsubsection{Data Augmentation}

To improve model generalization and increase the effective training dataset size, data augmentation techniques were applied:
\begin{itemize}
    \item \textbf{Noise injection:} Small random noise added to training samples
    \item \textbf{Jittering:} Slight perturbations to feature values
    \item \textbf{Scaling:} Random scaling of feature magnitudes
\end{itemize}

These augmentation methods doubled the effective training size (2x augmentation factor), helping to reduce overfitting and improve model robustness.

\subsubsection{Feature Scaling}

Two scaling approaches were evaluated:
\begin{itemize}
    \item \textbf{MinMax Scaler:} Scales features to [0, 1] range
    \item \textbf{Standard Scaler:} Standardizes features to zero mean and unit variance
\end{itemize}

The standard scaler combined with the long lag configuration yielded the best results for the recurrent models.

\subsubsection{Model Training Configuration}

All deep learning models were trained with the following configuration:
\begin{itemize}
    \item \textbf{Optimizer:} Adam
    \item \textbf{Loss Function:} Mean Squared Error (MSE)
    \item \textbf{Batch Size:} Typically 32 or 64
    \item \textbf{Epochs:} Training continued for multiple epochs until early stopping criteria were met
    \item \textbf{Weight Initialization:} Xavier/Glorot initialization (TensorFlow defaults)
\end{itemize}

\subsubsection{Regularization and Stability}

To avoid overfitting, several regularization techniques were applied:
\begin{itemize}
    \item Dropout layers were included in the LSTM and GRU models
    \item Batch normalization was applied where appropriate
    \item Early stopping was used to monitor validation loss, and training automatically stopped when the validation loss stopped improving for several consecutive epochs
\end{itemize}

\subsubsection{Model-Specific Training Notes}

\begin{itemize}
    \item \textbf{LSTM:} Processed sequential inputs with a 24-hour lookback window. Achieved strong performance with RMSE of 0.314 and R\textsuperscript{2} of 0.857, demonstrating effective capability in capturing temporal patterns in the load data.
    \item \textbf{GRU:} Provided a computationally lighter alternative to LSTM with slightly better performance, achieving RMSE of 0.289 and R\textsuperscript{2} of 0.879. The GRU architecture proved most effective among the deep learning models for this forecasting task.
\end{itemize}

\subsection{\sizexii Validation Approach}

A consistent evaluation strategy was applied across all models:

\textbf{Machine Learning Models:}
\begin{itemize}
    \item Validated using GridSearchCV with five-fold cross-validation
    \item Best hyperparameters were chosen based on the minimum validation loss
\end{itemize}

\textbf{Deep Learning Models:}
\begin{itemize}
    \item Validated using a 15 percent validation split (chronologically ordered)
    \item Early stopping was used to prevent overfitting, with patience of approximately 10 epochs
    \item Best-performing model weights were preserved through checkpointing
    \item Multiple lag configurations and scaling methods were systematically compared
\end{itemize}

\section{\sizexii Performance Evaluation}

To compare ML and DL models fairly, the same set of metrics was used for all of
them. These numbers show how close the predictions are to the actual values:
\begin{enumerate}
    \item Mean Absolute Error (MAE)
    \item Root Mean Squared Error (RMSE)
    \item Mean Absolute Percentage Error (MAPE)
    \item Coefficient of Determination (R\textsuperscript{2})
\end{enumerate}

Together, they measure accuracy, stability, and overall forecasting quality.

\subsection{\sizexii Mean Absolute Error (MAE)}

MAE gives the average size of prediction errors, ignoring their direction. It is easy
to understand and less sensitive to occasional big misses.

\textbf{Formula:}
\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}
where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the
prediction.

\textbf{What it means:} Lower MAE means predictions stay close to reality on average.

\subsection{\sizexii Root Mean Squared Error (RMSE)}

RMSE is one of the most common metrics in load forecasting. It stays in the same
units as the original data (MW) and punishes big errors harder than small ones.

\textbf{Formula:}
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}
where $n$ is the count, $y_i$ is actual load, and $\hat{y}_i$ is predicted load.

\textbf{What it means:} Lower RMSE indicates better overall accuracy. Because errors
are squared, large misses hurt the score more than with MAE.

\subsection{\sizexii Mean Absolute Percentage Error (MAPE)}

MAPE shows errors as a percentage of actual values, making it easy to compare
performance across feeders or demand levels.

\textbf{Formula:}
\begin{equation}
\text{MAPE} = \frac{100}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\end{equation}
where $n$ is the sample count, $y_i$ is actual load, and $\hat{y}_i$ is the forecast.

\textbf{What it means:} Lower MAPE is better. The formula breaks when actual load is
zero, but that never happened here since the feeder always runs.

\subsection{\sizexii Coefficient of Determination (R\textsuperscript{2} Score)}

R\textsuperscript{2} tells how much of the variation in load the model explains.

\textbf{Formula:}
\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}
where $y_i$ is actual, $\hat{y}_i$ is predicted, and $\bar{y}$ is the mean of all actuals.

\textbf{What it means:} R\textsuperscript{2} of 1 means perfect predictions; 0 means the
model is no better than just guessing the average. Higher is better, though negative
values are possible if the model is really bad.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% END OF CHAPTER THREE %%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% CHAPTER FOUR %%%%%%%%%%%%%%%%%%%%

\chapter*{\vspace{-1in}\centering {\sizexii CHAPTER FOUR: RESULTS AND DISCUSSION}}
\addcontentsline{toc}{chapter}{\textbf{CHAPTER FOUR: RESULTS AND DISCUSSION}}
\setcounter{chapter}{4}
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\vspace{-12pt}

This chapter shows how all the ML and DL models performed on the Baneshwor Feeder
forecasting task. Every model was measured with the same metrics (MAE, RMSE,
MAPE, R\textsuperscript{2}) so the comparison stays fair.

\section{\sizexii Exploratory Data Analysis}

Before building models, the data was explored to understand patterns, spot outliers,
and check feature relationships. The merged dataset covers the full study period at
hourly resolution, including MW readings, temperature, solar radiation, and humidity,
plus the derived time features (hour, day, month, etc.) and their sine-cosine
encodings. Quality checks confirmed no remaining gaps, no half-hour rows, and proper
alignment between load and weather records.

Hourly MW values were averaged by day and plotted across the study period. The
trend shows clear daily, weekly, and seasonal swings. Winter months have lower solar
radiation and slightly different load behavior. Occasional dips match known outages or
holidays. Solar radiation follows a strong daytime arc, which ties into its moderate
correlation with load. Overall, the Baneshwor Feeder behaves like a typical mixed-use
distribution feeder---strong daily cycles plus some seasonal variation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/lineGraphLoad.png}
    \caption{Daily Average Electricity Load Over Time}
    \label{fig:line_graph_load}
\end{figure}

The hourly load values were averaged across the full dataset to understand the feeder's daily consumption pattern. The analysis showed that the minimum load typically occurs around 3:00 AM, which reflects low residential and commercial activity during that time. Load levels begin to rise through the morning and reach a peak at around 19:00, with the average peak load reaching approximately 3.16 MW. This aligns with evening lighting needs and heightened residential usage. A boxplot comparing load against hour of day further illustrated that evening hours exhibit higher variance, while midnight to early-morning hours display more stable and lower demand. These observations confirm that the hour of the day is one of the strongest predictors of load in this feeder.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/hourlyLoad.png}
    \caption{Load Distribution by Hour}
    \label{fig:hourly_load}
\end{figure}

Monthly averages revealed that warmer months experience higher temperatures, although the corresponding load behavior varies across the year. Seasonal patterns are present but not as dominant as the daily cycles observed in the feeder. Consumption typically increases during festival seasons when household activity rises. These seasonal shifts are effectively captured through the Month feature and its corresponding cyclical encodings.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/averageload.png}
    \caption{Average Load by Month}
    \label{fig:average_load_month}
\end{figure}

All weather variables were examined individually to understand their behavior and potential influence on load. Air temperature ranged from roughly 1°C to 33°C and followed a clear daily cycle, with warmer afternoons and cooler nights. Global solar radiation showed a distinct daytime-only pattern, peaking sharply around midday and dropping to zero during nighttime hours. Relative humidity tended to be higher during nighttime and rainy months and displayed a slight inverse relationship with temperature. The temperature--load relationship showed a weak positive correlation, with load increasing moderately as temperatures rise, which is typical for mixed-load areas where fans and cooling appliances see greater use. Solar radiation displayed a moderate positive correlation with load, as higher midday radiation often coincides with active residential and commercial activity. Humidity exhibited a weak negative correlation, since high humidity is generally associated with cloudy or rainy conditions during which daytime load may decrease slightly.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/timeAirTemp.png}
    \caption{Air Temperature Variation Over the Study Period}
    \label{fig:time_air_temp}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/timeRadiation.png}
    \caption{Global Solar Radiation Variation Over the Study Period}
    \label{fig:time_radiation}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/timeHumidity.png}
    \caption{Relative Humidity Variation Over the Study Period}
    \label{fig:time_humidity}
\end{figure}

\subsection{\sizexii Outlier Treatment}

During the data quality assessment phase, outliers in the MW (Megawatt) column were identified and treated. For the Baneshwor Feeder, double-digit MW values (i.e., MW $\geq$ 10) are technically infeasible given the feeder's capacity and operational characteristics. Such extreme values likely represent sensor errors, data entry mistakes, or measurement anomalies rather than actual consumption.

A total of 56 records with MW values $\geq$ 10 were identified as outliers, representing approximately 0.23\% of the total dataset. These outlier values ranged from 10.30 MW to 404.00 MW, which are clearly outside the feasible operating range for this feeder. A hard threshold approach was employed to remove all records with MW $\geq$ 10 MW.

Figure~\ref{fig:boxplot_outlier} presents boxplots of the MW column before and after outlier removal. The left panel shows the original distribution with extreme outliers extending up to 404 MW, while the right panel displays the cleaned distribution with all MW values within the feasible single-digit range (0.00 to 8.50 MW). After outlier treatment, the cleaned dataset contained 24,352 records suitable for model training and evaluation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/boxplot_comparison_before_after.png}
    \caption{Boxplot of MW Column Before and After Outlier Removal}
    \label{fig:boxplot_outlier}
\end{figure}

In addition to active power, the exogenous weather variables were also screened for anomalous values. Relative humidity readings below physically realistic levels and a few saturated values above the upper bound were removed, yielding a more compact distribution centered between roughly 60\% and 100\%. Air temperature exhibited a small number of extreme low and high values that were inconsistent with the local climate; these were treated using the same boxplot-based rule, resulting in a stable range of approximately 1\textcelsius{} to 33\textcelsius{}. Global solar radiation initially contained several unrealistically high spikes (exceeding 1000~W/m\textsuperscript{2}), which were removed so that the cleaned series remained within a plausible envelope of about 0--800~W/m\textsuperscript{2}. Figures~\ref{fig:humidity_outlier}--\ref{fig:solar_outlier} summarize the effect of the outlier removal procedure on these three weather variables, where the left panel shows the original distribution and the right panel shows the cleaned distribution used for model training.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/relativeHumidity.png}
    \caption{Relative Humidity -- Outlier Removal Comparison}
    \label{fig:humidity_outlier}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/airTemperature.png}
    \caption{Air Temperature -- Outlier Removal Comparison}
    \label{fig:air_temp_outlier}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/globalSolar.png}
    \caption{Global Solar Radiation -- Outlier Removal Comparison}
    \label{fig:solar_outlier}
\end{figure}

Table~\ref{tab:mw_stats} presents the descriptive statistics of the MW column after outlier removal. The mean load of 2.35 MW and median of 2.30 MW indicate a relatively symmetric distribution. The standard deviation of 0.92 MW reflects moderate variability in hourly demand, while the minimum (0.00 MW) and maximum (8.50 MW) values confirm that all records now fall within the technically feasible range for the Baneshwor Feeder.

\begin{table}[H]
\centering
\caption{Descriptive Statistics of MW Column (After Outlier Removal)}
\label{tab:mw_stats}
\begin{tabular}{ll}
\hline
\textbf{Statistic} & \textbf{Value} \\
\hline
Mean & 2.3483 MW \\
Median & 2.3000 MW \\
Standard Deviation & 0.9186 MW \\
Variance & 0.8438 MW\textsuperscript{2} \\
Minimum & 0.0000 MW \\
Maximum & 8.5000 MW \\
\hline
\end{tabular}
\end{table}

\section{\sizexii Model Performance Results}

\subsection{\sizexii Machine Learning Model Performance}

All machine learning models were trained on the final feature-engineered dataset and evaluated on the test set. To optimize model performance, hyperparameter tuning was conducted using GridSearchCV with five-fold cross-validation. Table~\ref{tab:ml_hyperparams} summarizes the hyperparameter search space explored for each machine learning model. The best-performing configurations were selected based on the lowest validation error.

\begin{table}[H]
\centering
\caption{Hyperparameter Search Space for Machine Learning Models}
\label{tab:ml_hyperparams}
\begin{tabular}{ll}
\hline
\textbf{Model} & \textbf{Hyperparameters} \\
\hline
Ridge Regression & $\alpha$ = [0.001, 0.01, 0.1, 1, 10, 100] \\
\hline
Random Forest & 
\begin{tabular}[c]{@{}l@{}}
Number of trees = [100, 200] \\
Max depth = [10, 15, 20] \\
Min samples split = [2, 5] \\
Min samples leaf = [1, 2]
\end{tabular} \\
\hline
Gradient Boosting &
\begin{tabular}[c]{@{}l@{}}
Estimators = [100, 150, 200] \\
Learning rate = [0.05, 0.1, 0.15] \\
Max depth = [3, 5, 7]
\end{tabular} \\
\hline
XGBoost &
\begin{tabular}[c]{@{}l@{}}
Estimators = [100, 200] \\
Max depth = [4, 6, 8] \\
Learning rate = [0.05, 0.1] \\
Subsample = [0.8, 1.0]
\end{tabular} \\
\hline
SVR &
\begin{tabular}[c]{@{}l@{}}
C = [1, 10, 100] \\
Gamma = [scale, 0.01, 0.1] \\
Epsilon = [0.01, 0.1, 0.5]
\end{tabular} \\
\hline
\end{tabular}
\end{table}

After tuning, the models were evaluated on the test set. Table~\ref{tab:ml_results} presents the performance of all machine learning models.

\begin{table}[H]
\centering
\caption{Machine Learning Models Evaluation Matrix}
\label{tab:ml_results}
\begin{tabular}{|c|l|c|c|c|c|}
\hline
\textbf{Sn.No.} & \textbf{Model} & \textbf{MAE} & \textbf{RMSE} & \textbf{MAPE} & \textbf{R²} \\
\hline
1 & XGBoost (Tuned) & 0.257 & 0.384 & 12.693 & 0.831 \\
\hline
2 & Random Forest (Tuned) & 0.294 & 0.435 & 14.290 & 0.783 \\
\hline
3 & Random Forest & 0.305 & 0.444 & 14.825 & 0.774 \\
\hline
4 & XGBoost & 0.313 & 0.449 & 15.242 & 0.769 \\
\hline
5 & Gradient Boosting & 0.330 & 0.469 & 16.062 & 0.749 \\
\hline
6 & SVR & 0.318 & 0.483 & 15.193 & 0.732 \\
\hline
7 & Ridge Regression & 0.502 & 0.649 & 25.021 & 0.518 \\
\hline
8 & Linear Regression & 0.502 & 0.649 & 25.021 & 0.518 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Discussion of Results}

All ML models were scored with MAE, RMSE, MAPE, and R\textsuperscript{2}. Linear and
Ridge Regression served as baselines; their high errors and low R\textsuperscript{2}
show they cannot handle the nonlinear patterns in the load data. SVR did better,
cutting errors noticeably, but it still trailed the tree-based methods.

Random Forest and Gradient Boosting pulled ahead on every metric, capturing
complex relationships that simpler models miss. Tuned XGBoost topped the ML pack
with RMSE 0.384, R\textsuperscript{2} 0.831, and MAPE 12.69\%. Its edge comes from
gradient-based learning, built-in regularization, and smart handling of feature
interactions.

From a market standpoint, a MAPE around 12--13\% is a solid step up from basic
statistics and is workable for hour-ahead intraday adjustments \cite{weron2014electricity}.
Every percentage-point improvement means lower imbalance bills over time. The
steady R\textsuperscript{2} across peak and off-peak hours shows the model can be
trusted when stakes are high.

Bottom line: tree-based ensembles---especially XGBoost---give the best ML results for
feeder-level short-term forecasting and fit well with market-operation needs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/xboostActual.png}
    \caption{XBoost (Tuned) Actual vs Predicted}
    \label{fig:xboost_actual}
\end{figure}

\subsection{\sizexii Deep Learning Model Performance}

Deep learning models were trained using lag features and sliding-window sequences to capture temporal dependencies in the feeder load data. Two recurrent architectures were implemented: LSTM and GRU. Comprehensive experiments were conducted across multiple lag configurations and scaling methods to identify optimal configurations. Table~\ref{tab:dl_hyperparams} presents the hyperparameter and configuration search space explored for the deep learning models.

\begin{table}[H]
\centering
\caption{Hyperparameter Search Space for Deep Learning Models}
\label{tab:dl_hyperparams}
\begin{tabular}{ll}
\hline
\textbf{Component} & \textbf{Values} \\
\hline
\multicolumn{2}{c}{\textbf{Lag Feature Configurations}} \\
\hline
Short lags & [1, 3, 6] hours \\
Medium lags & [1, 3, 6, 12, 24] hours \\
Long lags & [1, 3, 6, 12, 24, 48] hours \\
\hline
\multicolumn{2}{c}{\textbf{Feature Scaling Methods}} \\
\hline
MinMax Scaler & Scales to [0, 1] range \\
Standard Scaler & Zero mean, unit variance \\
\hline
\multicolumn{2}{c}{\textbf{LSTM / GRU Architecture}} \\
\hline
Hidden units & [32, 64, 128] \\
Dropout rate & [0.0, 0.1, 0.2] \\
Activation function & ReLU \\
\hline
\multicolumn{2}{c}{\textbf{Sequence Modeling (LSTM/GRU)}} \\
\hline
Look-back window & 24 hours \\
Batch size & 32 \\
\hline
\multicolumn{2}{c}{\textbf{Training Parameters}} \\
\hline
Learning rate & 0.001 \\
Optimizer & Adam \\
Max epochs & 100 \\
Early stopping patience & 10 epochs \\
Data augmentation & 2x (noise, jittering, scaling) \\
\hline
\end{tabular}
\end{table}

After conducting extensive training using the cleaned and augmented dataset, the models were evaluated using the same metrics as the ML models. The best results obtained for each deep learning architecture are presented in Table~\ref{tab:dl_results}. The GRU model with long lag configuration [1, 3, 6, 12, 24, 48] and standard scaling achieved the best overall performance among the deep learning models.

\begin{table}[H]
\centering
\caption{Deep Learning Models Evaluation Matrix}
\label{tab:dl_results}
\begin{tabular}{|c|l|c|c|c|c|}
\hline
\textbf{Sn.No.} & \textbf{Model} & \textbf{MAE} & \textbf{RMSE} & \textbf{MAPE} & \textbf{R²} \\
\hline
1 & GRU & 0.192 & 0.289 & 7.364 & 0.879 \\
\hline
2 & LSTM & 0.205 & 0.314 & 7.612 & 0.857 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Discussion of Results}

Both LSTM and GRU were measured with the same metrics. GRU came out on top with
MAE 0.192, RMSE 0.289, MAPE 7.36\%, and R\textsuperscript{2} 0.879. LSTM was close
behind at RMSE 0.314 and R\textsuperscript{2} 0.857. Both R\textsuperscript{2} scores
above 0.85 show the recurrent networks picked up the time-dependent patterns well.

GRU edged ahead partly because its simpler gate structure works efficiently here
with fewer parameters and faster training. Both models learned the sliding-window
sequences and benefited from the long lag setup [1, 3, 6, 12, 24, 48 hours], which
gives them explicit memory of what happened at different horizons.

For market use, MAPE around 7\% sits comfortably in the 5--10\% range that most
intraday markets consider acceptable \cite{lago2021dayahead}. That level of accuracy
can cut imbalance costs noticeably. The multi-scale lag features help the model
track hour-to-hour and day-to-day patterns---exactly what utilities need when
adjusting positions on short notice.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/gruActual.png}
    \caption{GRU Model Actual vs Predicted Values (Last 200 Test Points)}
    \label{fig:gru_actual}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% END OF CHAPTER FOUR %%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% CHAPTER FIVE %%%%%%%%%%%%%%%%%%%%

\chapter*{\vspace{-1in}\centering {\sizexii CHAPTER FIVE: CONCLUSION}}
\addcontentsline{toc}{chapter}{\textbf{CHAPTER FIVE: CONCLUSION}}
\setcounter{chapter}{5}
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\vspace{-12pt}

\section{\sizexii Conclusion}

This thesis built and tested a complete short-term load forecasting system for the
Baneshwor Feeder using both ML and DL methods. The work followed a clear
pipeline: gather and clean the data, engineer features, train models, tune
hyperparameters, and compare results with standard metrics. The whole framework
aims at the hour-ahead horizon---the fast-moving intraday window where utilities
need to adjust positions quickly.

In competitive electricity markets, getting the forecast right matters financially.
Day-ahead markets need predictions a day in advance for unit commitment; intraday
markets need hour-ahead numbers for position tweaks and balancing. The models here
are designed with those timescales in mind, helping cut imbalance costs and keep
supply and demand aligned \cite{weron2014electricity, zareipour2010electricity}.

Results show that both ML and DL can hit accuracy levels that work for market
participation. On the ML side, tuned XGBoost scored RMSE 0.384, R\textsuperscript{2}
0.831, and MAPE 12.69\%. Random Forest and Gradient Boosting were not far behind,
confirming that tree-based ensembles handle feeder-level demand well.

On the DL side, both recurrent models landed below 8\% MAPE, which fits the
acceptable range for practical intraday use \cite{lago2021dayahead}. GRU performed
best overall: RMSE 0.289, R\textsuperscript{2} 0.879, MAPE 7.36\%. LSTM came close
at RMSE 0.314 and R\textsuperscript{2} 0.857. The long lag setup [1, 3, 6, 12, 24,
48 hours] helped both networks capture the hour-ahead patterns that intraday
trading relies on.

In short, this work proves that careful feature engineering combined with either
tuned boosted trees or well-configured recurrent networks can deliver forecasts
accurate enough for hour-ahead market operations. Choosing between ML and DL
comes down to data size, compute budget, and operational needs.

As Nepal moves toward more market-driven power trading and cross-border exchange,
these hour-ahead forecasting tools lay groundwork for efficient participation.
Better forecasts translate to less wasted hydropower, lower import bills, and a
stronger position in regional energy trade---benefits that ripple through the
whole economy.

\section{\sizexii Research Limitations}

Despite the solid results, a few limitations are worth noting:

\begin{enumerate}
    \item \textbf{Data size:} More years of data could sharpen seasonal learning and
    improve generalization.
    \item \textbf{Noisy feeder behavior:} Outages, spikes, and irregular events can
    throw off DL training when extra context is missing.
    \item \textbf{Weather granularity:} Hourly weather was all that was available;
    finer data or extra variables (wind, rain intensity) might help.
    \item \textbf{No live deployment:} The study stayed in an offline setting;
    real-time integration with NEA systems was not attempted.
    \item \textbf{No actual market testing:} The framework fits market timelines, but
    the models have not been tried in real trading situations where price swings and
    bidding rules add extra complexity.
\end{enumerate}

These points should be kept in mind when applying or extending the work.

\section{\sizexii Implications}

The findings carry practical takeaways for utilities, planners, and market players:

\begin{enumerate}
    \item \textbf{Ready for feeder-level use:} Tuned XGBoost (R\textsuperscript{2} 0.831,
    MAPE 12.69\%) and GRU (R\textsuperscript{2} 0.879, MAPE 7.36\%) both deliver
    low-error forecasts suitable for scheduling and peak management.
    \item \textbf{Market-ready accuracy:} GRU's 7.36\% MAPE sits comfortably in the
    range for intraday market operations, supporting position adjustments and
    imbalance control as Nepal's sector opens up \cite{weron2014electricity}.
    \item \textbf{Feature engineering pays off:} Cyclical encodings, lag variables, and
    weather features boosted all models, proving that domain knowledge matters.
    \item \textbf{Recurrent networks work:} Both GRU and LSTM captured time patterns
    well, confirming that RNN-style models fit load forecasting when set up right.
    \item \textbf{Foundation for market tools:} The forecasts can feed demand-side
    programs, smart-grid logic, load-shifting plans, and cross-border trading
    decisions.
    \item \textbf{Scalable pipeline:} The same workflow can extend to other NEA feeders
    with minor tweaks, enabling network-wide aggregation.
\end{enumerate}

\section{\sizexii Recommendation}

While the forecasting models work well, several paths could push accuracy and
applicability further:

\begin{enumerate}
    \item \textbf{Add more inputs:} Holidays, special events, price signals, and
    economic indicators could capture demand swings that weather and time features
    alone miss.
    \item \textbf{Try newer architectures:} Attention layers, transformers, or CNN-RNN
    hybrids might squeeze extra points out of hour-ahead predictions.
    \item \textbf{Go live:} Deploy the best model (GRU or XGBoost) in a real-time
    environment with online learning so it adapts as load patterns shift.
    \item \textbf{Link to market tools:} As Nepal opens its power market, connect
    forecasts to bidding algorithms and decision-support systems that turn
    predictions into trading actions \cite{conejo2010decision}.
    \item \textbf{Add uncertainty estimates:} Probabilistic forecasts (confidence
    bands, quantile outputs) would help utilities manage risk when prices swing
    \cite{weron2014electricity}.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% END OF CHAPTER FIVE %%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%%%%

\renewcommand{\bibname}{\vspace{-1in}\centering \sizexii \textbf{REFERENCES}}
\addcontentsline{toc}{chapter}{\normalfont REFERENCES}


%\bibliographystyle{apalike}
\setlength{\bibhang}{0pt}
%\bibliography{references}
\printbibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% END OF REFERENCES %%%%%%%%%%%%%%%%%%


\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% END OF DOCUMENT %%%%%%%%%%%%%%%%%%%
